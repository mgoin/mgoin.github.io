<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Michael Goin - Systems engineer focused on LLM inference performance">
  <title>Michael Goin</title>
  <link rel="stylesheet" href="style.css">
</head>

<body>
  <canvas id="sim" aria-hidden="true"></canvas>
  <button class="theme-toggle" aria-label="Toggle theme" title="Toggle light/dark">
    <svg class="icon-sun" viewBox="0 0 24 24" width="18" height="18" fill="none" stroke="currentColor" stroke-width="2">
      <circle cx="12" cy="12" r="5" />
      <path
        d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42" />
    </svg>
    <svg class="icon-moon" viewBox="0 0 24 24" width="18" height="18" fill="none" stroke="currentColor"
      stroke-width="2">
      <path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z" />
    </svg>
  </button>

  <main>
    <header>
      <div class="photo-container">
        <img src="assets/me.jpg" alt="Michael Goin" class="photo" width="120" height="120">
      </div>
      <h1>Michael Goin <code>@mgoin</code></h1>
      <p class="tagline">systems engineer making inference fast</p>
      <nav class="links">
        <a href="https://github.com/mgoin">github</a>
        <a href="https://x.com/mgoin_">twitter</a>
        <a href="https://www.linkedin.com/in/michael-goin/">linkedin</a>
        <a href="https://huggingface.co/mgoin">huggingface</a>
        <a href="https://scholar.google.com/citations?user=YfmwSv8AAAAJ">scholar</a>
      </nav>
    </header>

    <section id="about">
      <h2>About</h2>
      <p>
        I've been working in ML inference since 2019, currently focused on making SOTA open-source LLMs run fast on
        various hardware accelerators in <a href="https://github.com/vllm-project/vllm">vLLM</a>.
      </p>
      <p>
        I like working across the stack wherever the bottleneck is -
        CPU, GPU, compute-bound, memory-bound, io-bound by using Python, PyTorch, C++, CUDA.
        Most of my time goes into profiling, benchmarking, and figuring out why things are slow.
      </p>
      <p>
        Before that, my background was in HPC where I worked on robotics, materials science simulations, and <a
          href="https://neuromorphic.eecs.utk.edu/">neuromorphic computing</a>.
      </p>
      <p>
        I'm currently working at <a href="https://www.redhat.com">Red Hat</a> on
        <a href="https://github.com/vllm-project/vllm">vLLM</a> to power the open-source AI ecosystem with fast and easy
        inference.
        Before acquisition by Red Hat, I was at <a href="https://www.neuralmagic.com">Neural Magic</a>, where I worked
        on vLLM and originally
        built a
        <a href="https://github.com/neuralmagic/deepsparse">sparsity-aware inference compiler</a> that optimized CNNs,
        Transformers, and other models for CPUs.
      </p>
      <p>
        If you want to reach me, the best way is to ping me <code>@mgoin</code> on <a href="https://slack.vllm.ai">vLLM
          Slack</a>. I'm always happy to collaborate on projects or ideas related to inference performance!
      </p>
    </section>

    <section id="work">
      <h2>Work</h2>
      <ul class="timeline">
        <li>
          <span class="date">2025-01 -> now</span>
          <span class="role"><a href="https://www.redhat.com">Red Hat</a> (<a
              href="https://www.redhat.com/en/about/press-releases/red-hat-acquire-neural-magic">acq Neural Magic</a>),
            Principal Engineer</span>
        </li>
        <li>
          <span class="date">2024-01 -> now</span>
          <span class="role"><a href="https://github.com/vllm-project/vllm">vLLM</a>, Core Maintainer</span>
        </li>
        <li>
          <span class="date">2019-09 -> 2024-12</span>
          <span class="role"><a href="https://www.neuralmagic.com">Neural Magic</a>, Engineering Tech Lead</span>
        </li>
      </ul>
    </section>

    <section id="changelog">
      <h2>Changelog</h2>
      <p class="section-note">Things I've shipped or helped ship.</p>
      <ul class="log">
        <!-- Add new entries at the top -->
        <li>
          <span class="date">2025-10-09</span>
          <span class="entry"><a href="https://blog.vllm.ai/2025/10/09/blackwell-inferencemax.html">vLLM + NVIDIA
              Blackwell Optimized Inference</a></span>
        </li>
        <li>
          <span class="date">2025-01-27</span>
          <span class="entry"><a href="https://blog.vllm.ai/2025/01/27/v1-alpha-release.html">vLLM V1: A Major Upgrade
              to vLLM's Core Architecture</a></span>
        </li>
        <li>
          <span class="date">2025-01-14</span>
          <span class="entry"><a href="https://blog.vllm.ai/2025/01/14/struct-decode-intro.html">Structured Decoding
              Optimizations in vLLM</a></span>
        </li>
        <li>
          <span class="date">2024-08-31</span>
          <span class="entry">Accurate Compression of Text-to-Image Diffusion Models via Vector Quantization (<a
              href="https://arxiv.org/abs/2409.00492">arXiv</a>)</span>
        </li>
        <li>
          <span class="date">2024-06-20</span>
          <span class="entry">Won a bounty converting nvidia/Nemotron-4-340B-Instruct to work with vLLM (<a
              href="https://x.com/natolambert/status/1814735390877884823">twitter</a>)</span>
        </li>
        <li>
          <span class="date">2024-05-06</span>
          <span class="entry">Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and
            Deployment (<a href="https://arxiv.org/abs/2405.03594">arXiv</a>)</span>
        </li>
        <li>
          <span class="date">2023-10-10</span>
          <span class="entry">Sparse Fine-tuning for LLM Inference Acceleration (<a
              href="https://arxiv.org/abs/2310.06927">arXiv</a>)</span>
        </li>
        <li>
          <span class="date">2023-10-10</span>
          <span class="entry">The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large
            Language Models (<a href="https://arxiv.org/abs/2203.07259">arXiv</a>)</span>
        </li>
        <!-- Add more entries here -->
      </ul>
    </section>

    <section id="talks">
      <h2>Talks</h2>
      <ul class="talks-list">
        <!-- Add more entries here -->
        <li>
          <span class="date">2024 -> now</span>
          <span class="entry"><a href="https://www.youtube.com/playlist?list=PLbMP1JcGBmSHxp4-lubU5WYmJ9YgAQcf3">vLLM
              Office Hours</a>, ~bi-weekly vLLM update I host with guests from the community. Slides are available in
            each video's description.</span>
        </li>
        <li>
          <span class="date">2025-11-06</span>
          <span class="entry"><a href="https://luma.com/0gls27kb">vLLM Zurich Meetup</a>,
            <a
              href="https://docs.google.com/presentation/d/1UC9PTLCHYXQpOmJDSFg6Sljra3iVXzc09DeEI7dnxMc/edit?usp=sharing">Slides</a>,
            <a href="https://www.youtube.com/watch?v=6m6ZE6yVEDI">Recording</a></span>
        </li>
        <li>
          <span class="date">2025-11-01</span>
          <span class="entry"><a href="https://mp.weixin.qq.com/s/xSrYXjNgr1HbCP4ExYNG1w">vLLM Beijing Meetup</a>, <a
              href="https://drive.google.com/drive/folders/1nQJ8ZkLSjKxvu36sSHaceVXtttbLvvu-?usp=drive_link">Slides</a></span>
        </li>
        <li>
          <span class="date">2025-10-21</span>
          <span class="entry"><a href="https://openagentsummit2025.sched.com/event/28zVb">PyTorch Conf 2025 -
              Accelerating Open-Source RL
              and Agentic Inference with vLLM</a>, <a
              href="https://docs.google.com/presentation/d/1HzuszVupkE5tzVKYe2IvOBwz02YMnKcUuHm19vUR2SI/edit?usp=sharing">Slides</a>,
            <a href="https://youtu.be/AZcJJbBBgLc">Recording</a></span>
        </li>
        <li>
          <span class="date">2025-10-09</span>
          <span class="entry"><a href="https://luma.com/0zuf1wp5">vLLM Tokyo Meetup</a>, <a
              href="https://drive.google.com/file/d/1bcrrAE1rxUgx0mjIeOWT6hNe2RefC5Hm/view">Slides</a></span>
        </li>
        <li>
          <span class="date">2025-09-18</span>
          <span class="entry"><a href="https://luma.com/vjfelimw">vLLM Boston Meetup</a>, <a
              href="https://docs.google.com/presentation/d/1k3EBinP85fN7x2glLURCEMheT4FJufJOgYiBaL8ZwZI/edit?usp=sharing">Slides</a></span>
        </li>
        <li>
          <span class="date">2025-05-07</span>
          <span class="entry"><a href="https://lu.ma/c1rqyf1f">NYC vLLM Meetup</a>, <a
              href="https://docs.google.com/presentation/d/1_q_aW_ioMJWUImf1s1YM-ZhjXz8cUeL0IJvaquOYBeA/edit?usp=sharing">Slides</a></span>
        </li>
      </ul>
    </section>

    <footer>
      <p>Boston, MA</p>
      <p class="b" title="B says hi">B</p>
    </footer>
  </main>

  <script src="script.js"></script>
</body>

</html>